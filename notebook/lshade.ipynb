{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "[3.22722294e-05 9.36677822e-05]\n",
      "0.00012594001161723043\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import cauchy\n",
    "\n",
    "\n",
    "def fitness(individual):\n",
    "    return np.sum(individual)\n",
    "\n",
    "\n",
    "def weighted_lehmar_mean(succeding_rates, individual_fitnesses, trial_fitnesses):\n",
    "    improvements = individual_fitnesses - trial_fitnesses\n",
    "    total_improvement = np.sum(improvements)\n",
    "    weights = improvements / total_improvement\n",
    "\n",
    "    lehmar_mean = np.sum(weights * succeding_rates**2) / np.sum(\n",
    "        weights * succeding_rates\n",
    "    )\n",
    "\n",
    "    return lehmar_mean\n",
    "\n",
    "\n",
    "problem_dimensionality = 2\n",
    "generation_number = 1\n",
    "initial_population_size = 100\n",
    "population_size = initial_population_size\n",
    "archive_size = initial_population_size\n",
    "archive = np.zeros((archive_size, problem_dimensionality))\n",
    "archive_fitnesses = np.empty(archive_size)\n",
    "archive_fitnesses.fill(np.nan)\n",
    "\n",
    "lower_boundary = 0  # per ora keep boundary fixed for all dimensions\n",
    "upper_boundary = 1  # per ora keep boundary fixed for all dimensions\n",
    "\n",
    "memory_size = 10  # H\n",
    "memory = np.zeros((memory_size, 2)) + 0.5\n",
    "\n",
    "p = 0.2\n",
    "\n",
    "MEMORY_MCR_INDEX = 0\n",
    "MEMORY_MMR_INDEX = 1\n",
    "\n",
    "previous_gen_population = np.random.uniform(\n",
    "    lower_boundary,\n",
    "    upper_boundary,\n",
    "    size=(initial_population_size, problem_dimensionality),\n",
    ")\n",
    "\n",
    "\n",
    "max_generations = 100\n",
    "\n",
    "crossing_rates = np.zeros(population_size)\n",
    "mutation_rates = np.zeros(population_size)\n",
    "\n",
    "previous_gen_individuals_fitnesses = np.array(\n",
    "    [fitness(individual) for individual in previous_gen_population]\n",
    ")\n",
    "current_number_of_fitness_evaluations = population_size\n",
    "memory_index_to_update = 0\n",
    "\n",
    "max_number_fitness_evaluations = 1000\n",
    "min_number_of_individuals = 4\n",
    "\n",
    "best_individual_fitness_index = np.argmin(previous_gen_individuals_fitnesses)\n",
    "global_best_individual = previous_gen_population[best_individual_fitness_index]\n",
    "global_best_individual_fitness = previous_gen_individuals_fitnesses[\n",
    "    best_individual_fitness_index\n",
    "]\n",
    "\n",
    "while (generation_number <= max_generations) and (\n",
    "    current_number_of_fitness_evaluations < max_number_fitness_evaluations\n",
    "):\n",
    "\n",
    "    succeding_crossover_rates = np.empty(population_size)\n",
    "    succeding_crossover_rates.fill(np.nan)\n",
    "    succeding_mutation_rates = np.empty(population_size)\n",
    "    succeding_mutation_rates.fill(np.nan)\n",
    "    succeding_trials = np.empty((population_size, problem_dimensionality))\n",
    "    succeding_trial_fitnesses = np.empty(population_size)\n",
    "    succeding_trials_count = 0\n",
    "    trail_individuals = np.empty((population_size, problem_dimensionality))\n",
    "    trial_fitnesses = np.empty(population_size)\n",
    "    are_individuals_surpassed = np.zeros(population_size)\n",
    "    next_gen_individuals = previous_gen_population.copy()\n",
    "    next_gen_individuals_fitnesses = previous_gen_individuals_fitnesses.copy()\n",
    "\n",
    "    for individual_index in range(population_size):\n",
    "        memory_index = np.random.randint(0, memory_size)\n",
    "        if np.isnan(memory[memory_index, MEMORY_MCR_INDEX]):\n",
    "            crossing_rates[individual_index] = 0\n",
    "        else:\n",
    "            crossing_rates[individual_index] = np.clip(\n",
    "                np.random.normal(memory[memory_index, MEMORY_MCR_INDEX], 0.1), 0, 1.0\n",
    "            )\n",
    "\n",
    "        generated_mutation_rate = 0\n",
    "        while generated_mutation_rate <= 0:\n",
    "            generated_mutation_rate = np.clip(\n",
    "                cauchy.rvs(memory[memory_index, MEMORY_MMR_INDEX], 0.1), None, 1.0\n",
    "            )\n",
    "        mutation_rates[individual_index] = generated_mutation_rate\n",
    "\n",
    "        number_of_best_individuals = round(population_size * p)\n",
    "        random_best_individual_index = np.random.randint(0, number_of_best_individuals)\n",
    "\n",
    "        best_individuals_indices = np.argpartition(\n",
    "            previous_gen_individuals_fitnesses, number_of_best_individuals\n",
    "        )[:number_of_best_individuals]\n",
    "\n",
    "        best_individual = previous_gen_population[\n",
    "            best_individuals_indices[random_best_individual_index]\n",
    "        ]\n",
    "\n",
    "        number_individuals_in_archive = np.sum(~np.isnan(archive_fitnesses))\n",
    "        individual_indices_not_current = [\n",
    "            index\n",
    "            for index in range(population_size + number_individuals_in_archive)\n",
    "            if index != individual_index\n",
    "        ]\n",
    "        random_individual_indices = np.random.choice(\n",
    "            individual_indices_not_current, 2, replace=False\n",
    "        )\n",
    "\n",
    "        random_individual_1 = (\n",
    "            previous_gen_population[random_individual_indices[0]]\n",
    "            if random_individual_indices[0] < population_size\n",
    "            else archive[random_individual_indices[0] - population_size]\n",
    "        )\n",
    "        random_individual_2 = (\n",
    "            previous_gen_population[random_individual_indices[1]]\n",
    "            if random_individual_indices[1] < population_size\n",
    "            else archive[random_individual_indices[1] - population_size]\n",
    "        )\n",
    "        current_individual = previous_gen_population[individual_index]\n",
    "\n",
    "        mutant = (\n",
    "            current_individual\n",
    "            + mutation_rates[individual_index] * (best_individual - current_individual)\n",
    "            + mutation_rates[individual_index]\n",
    "            * (random_individual_1 - random_individual_2)\n",
    "        )\n",
    "\n",
    "        for gene_index in range(problem_dimensionality):\n",
    "            if mutant[gene_index] < lower_boundary:\n",
    "                mutant[gene_index] = (\n",
    "                    lower_boundary + current_individual[gene_index]\n",
    "                ) / 2\n",
    "            elif mutant[gene_index] > upper_boundary:\n",
    "                mutant[gene_index] = (\n",
    "                    upper_boundary + current_individual[gene_index]\n",
    "                ) / 2\n",
    "\n",
    "        random_gene_index_to_mutate = np.random.randint(0, problem_dimensionality)\n",
    "        crossover_gene_index = (\n",
    "            np.random.rand(problem_dimensionality) < crossing_rates[individual_index]\n",
    "        )\n",
    "        crossover_gene_index[random_gene_index_to_mutate] = True\n",
    "\n",
    "        trail_individual = np.where(crossover_gene_index, mutant, current_individual)\n",
    "        trail_individuals[individual_index] = trail_individual\n",
    "        trial_fitnesses[individual_index] = fitness(trail_individual)\n",
    "        current_number_of_fitness_evaluations = (\n",
    "            current_number_of_fitness_evaluations + 1\n",
    "        )\n",
    "\n",
    "    for individual_index in range(population_size):\n",
    "\n",
    "        if (\n",
    "            trial_fitnesses[individual_index]\n",
    "            <= previous_gen_individuals_fitnesses[individual_index]\n",
    "        ):\n",
    "            next_gen_individuals[individual_index] = trail_individuals[individual_index]\n",
    "            next_gen_individuals_fitnesses[individual_index] = trial_fitnesses[\n",
    "                individual_index\n",
    "            ]\n",
    "\n",
    "            if (\n",
    "                trial_fitnesses[individual_index]\n",
    "                < previous_gen_individuals_fitnesses[individual_index]\n",
    "            ):\n",
    "                if number_individuals_in_archive < archive_size:\n",
    "                    archive[number_individuals_in_archive] = trail_individuals[\n",
    "                        individual_index\n",
    "                    ]\n",
    "                    archive_fitnesses[number_individuals_in_archive] = trial_fitnesses[\n",
    "                        individual_index\n",
    "                    ]\n",
    "                else:\n",
    "                    random_element_index = np.random.randint(0, archive_size)\n",
    "                    archive[random_element_index] = trail_individuals[individual_index]\n",
    "                    archive_fitnesses[random_element_index] = trial_fitnesses[\n",
    "                        individual_index\n",
    "                    ]\n",
    "\n",
    "                succeding_mutation_rates[succeding_trials_count] = mutation_rates[\n",
    "                    individual_index\n",
    "                ]\n",
    "                succeding_crossover_rates[succeding_trials_count] = crossing_rates[\n",
    "                    individual_index\n",
    "                ]\n",
    "\n",
    "                succeding_trials[succeding_trials_count] = trail_individuals[\n",
    "                    individual_index\n",
    "                ]\n",
    "                succeding_trial_fitnesses[succeding_trials_count] = trial_fitnesses[\n",
    "                    individual_index\n",
    "                ]\n",
    "\n",
    "                are_individuals_surpassed[individual_index] = 1\n",
    "\n",
    "                succeding_trials_count += 1\n",
    "\n",
    "                if trial_fitnesses[individual_index] < global_best_individual_fitness:\n",
    "                    global_best_individual = trail_individuals[individual_index]\n",
    "                    global_best_individual_fitness = trial_fitnesses[individual_index]\n",
    "\n",
    "    if succeding_trials_count > 0:\n",
    "        if np.nanmax(succeding_mutation_rates) <= 0 or np.isnan(\n",
    "            memory[memory_index_to_update, MEMORY_MCR_INDEX]\n",
    "        ):\n",
    "            memory[memory_index_to_update, MEMORY_MCR_INDEX] = np.nan\n",
    "        else:\n",
    "            memory[memory_index_to_update, MEMORY_MCR_INDEX] = weighted_lehmar_mean(\n",
    "                succeding_crossover_rates[:succeding_trials_count],\n",
    "                previous_gen_individuals_fitnesses[are_individuals_surpassed == 1],\n",
    "                succeding_trial_fitnesses[:succeding_trials_count],\n",
    "            )\n",
    "\n",
    "        memory[memory_index_to_update, MEMORY_MMR_INDEX] = weighted_lehmar_mean(\n",
    "            succeding_mutation_rates[:succeding_trials_count],\n",
    "            previous_gen_individuals_fitnesses[are_individuals_surpassed == 1],\n",
    "            succeding_trial_fitnesses[:succeding_trials_count],\n",
    "        )\n",
    "\n",
    "        memory_index_to_update = (memory_index_to_update + 1) % memory_size\n",
    "\n",
    "    next_gen_population_size = round(\n",
    "        (\n",
    "            (min_number_of_individuals - initial_population_size)\n",
    "            / max_number_fitness_evaluations\n",
    "        )\n",
    "        * current_number_of_fitness_evaluations\n",
    "        + initial_population_size\n",
    "    )\n",
    "\n",
    "    if next_gen_population_size < population_size:\n",
    "        if next_gen_population_size < min_number_of_individuals:\n",
    "            next_gen_individuals = np.empty((0, 2))\n",
    "            next_gen_individuals_fitnesses = np.empty(0)\n",
    "        else:\n",
    "            number_of_individuals_to_delete = population_size - next_gen_population_size\n",
    "            worst_individuals_indices = np.argpartition(\n",
    "                next_gen_individuals_fitnesses, -number_of_individuals_to_delete\n",
    "            )[:number_of_individuals_to_delete]\n",
    "\n",
    "            next_gen_individuals = np.delete(\n",
    "                next_gen_individuals, worst_individuals_indices, axis=0\n",
    "            )\n",
    "            next_gen_individuals_fitnesses = np.delete(\n",
    "                next_gen_individuals_fitnesses, worst_individuals_indices\n",
    "            )\n",
    "            population_size = next_gen_population_size\n",
    "\n",
    "    previous_gen_population = next_gen_individuals\n",
    "    previous_gen_individuals_fitnesses = next_gen_individuals_fitnesses\n",
    "    print(generation_number)\n",
    "    generation_number = generation_number + 1\n",
    "\n",
    "\n",
    "print(global_best_individual)\n",
    "print(global_best_individual_fitness)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([], shape=(0, 2), dtype=float64)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.empty((0, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 7.81024968, 16.2788206 , 24.75883681])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rho_norms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1,  2,  0,  0,  0,  0],\n",
       "       [ 7,  8,  0,  0,  0,  0],\n",
       "       [13, 14,  0,  0,  0,  0]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "type_count = 2\n",
    "\n",
    "initial_population = np.array(\n",
    "    [[1, 2, 3, 4, 5, 6], [7, 8, 9, 10, 11, 12], [13, 14, 15, 16, 17, 18]]\n",
    ")\n",
    "\n",
    "for i in range(type_count):\n",
    "    rho_indices = [type_count + i * type_count + j for j in range(type_count)]\n",
    "    rho_values = initial_population[:, rho_indices]\n",
    "    rho_norms = np.linalg.norm(rho_values, axis=1)\n",
    "    rho_values = rho_values / rho_norms[:, np.newaxis]\n",
    "\n",
    "    initial_population[:, rho_indices] = rho_values\n",
    "\n",
    "initial_population\n",
    "\n",
    "# L'idea iu veloce Ã¨ lasciarle generate cosi e se hanno norma > 1 scalarle con epsilon drawn\n",
    "# from a uniform distribution in [0, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numba import njit\n",
    "from numba.typed import List\n",
    "\n",
    "\n",
    "@njit\n",
    "def negative_exponential_effect(\n",
    "    beta_mn: float, end_time: float, times_n: np.ndarray\n",
    ") -> float:\n",
    "    return np.sum(1 - np.exp(-beta_mn * (end_time - times_n)))\n",
    "\n",
    "\n",
    "@njit\n",
    "def loglikelihood_negative_exponential_contribution(\n",
    "    alphas_mn: np.ndarray,\n",
    "    betas_mn: np.ndarray,\n",
    "    end_time: float,\n",
    "    events_times: List[np.ndarray],\n",
    ") -> float:\n",
    "    num_events = len(events_times)\n",
    "    negative_exponential_effects = np.empty(num_events, dtype=np.float64)\n",
    "\n",
    "    for n in range(num_events):\n",
    "        negative_exponential_effects[n] = negative_exponential_effect(\n",
    "            betas_mn[n], end_time, events_times[n]\n",
    "        )\n",
    "\n",
    "    return np.sum(alphas_mn / betas_mn * negative_exponential_effects)\n",
    "\n",
    "\n",
    "@njit\n",
    "def r_mn(\n",
    "    beta_mn: float,\n",
    "    current_t_m: float,\n",
    "    previous_t_m: float,\n",
    "    previous_r_mn: float,\n",
    "    intermediate_t_n_values: np.ndarray,\n",
    ") -> float:\n",
    "    return np.exp(-beta_mn * (current_t_m - previous_t_m)) * previous_r_mn + np.sum(\n",
    "        np.exp(-beta_mn * (current_t_m - intermediate_t_n_values))\n",
    "    )\n",
    "\n",
    "\n",
    "@njit\n",
    "def counting_process_integral_subfunction(\n",
    "    mu_m: float,\n",
    "    alphas_mn: np.ndarray,\n",
    "    rs_mn: np.ndarray,\n",
    ") -> float:\n",
    "    return np.log(mu_m + np.sum(alphas_mn * rs_mn))\n",
    "\n",
    "\n",
    "@njit\n",
    "def loglikelihood_m(\n",
    "    m_index: int,\n",
    "    mu_m: float,\n",
    "    negative_exponential_contribution: float,\n",
    "    alphas_mn: np.ndarray,\n",
    "    betas_mn: np.ndarray,\n",
    "    end_time: float,\n",
    "    events_times: List[np.ndarray],\n",
    ") -> float:\n",
    "    num_events = len(events_times)\n",
    "\n",
    "    dt_integral = -mu_m * end_time - negative_exponential_contribution\n",
    "\n",
    "    recursive_part_effects = np.zeros(num_events, dtype=np.float64)\n",
    "    end_indices_times_events = np.zeros(num_events, dtype=np.float64)\n",
    "\n",
    "    for n in range(num_events):\n",
    "        end_indices_times_events[n] = np.searchsorted(\n",
    "            events_times[n], events_times[m_index][0], side=\"left\"\n",
    "        )\n",
    "\n",
    "        recursive_part_effects[n] = r_mn(\n",
    "            betas_mn[n],\n",
    "            events_times[m_index][0],\n",
    "            0,\n",
    "            recursive_part_effects[n],\n",
    "            events_times[n][: end_indices_times_events[n]],\n",
    "        )\n",
    "\n",
    "    counting_process_integral = counting_process_integral_subfunction(\n",
    "        mu_m,\n",
    "        alphas_mn,\n",
    "        recursive_part_effects,\n",
    "    )\n",
    "\n",
    "    for k in range(1, len(events_times[m_index])):\n",
    "        start_indices_times_events = end_indices_times_events.copy()\n",
    "\n",
    "        for n in range(num_events):\n",
    "            current_t_m = events_times[m_index][k]\n",
    "            previous_t_m = events_times[m_index][k - 1]\n",
    "\n",
    "            end_indices_times_events[n] = (\n",
    "                np.searchsorted(\n",
    "                    events_times[n][start_indices_times_events[n] :],\n",
    "                    current_t_m,\n",
    "                    side=\"left\",\n",
    "                )\n",
    "                + start_indices_times_events[n]\n",
    "            )\n",
    "\n",
    "            recursive_part_effects[n] = r_mn(\n",
    "                betas_mn[n],\n",
    "                current_t_m,\n",
    "                previous_t_m,\n",
    "                recursive_part_effects[n],\n",
    "                events_times[n][\n",
    "                    start_indices_times_events[n] : end_indices_times_events[n]\n",
    "                ],\n",
    "            )\n",
    "\n",
    "        counting_process_integral += counting_process_integral_subfunction(\n",
    "            mu_m,\n",
    "            alphas_mn,\n",
    "            recursive_part_effects,\n",
    "        )\n",
    "\n",
    "    return dt_integral + counting_process_integral\n",
    "\n",
    "\n",
    "@njit\n",
    "def loglikelihood(\n",
    "    mu: np.ndarray,\n",
    "    alphas: np.ndarray,\n",
    "    betas: np.ndarray,\n",
    "    end_time: float,\n",
    "    events_times: List[np.ndarray],\n",
    ") -> float:\n",
    "    num_events = len(events_times)\n",
    "    loglikelihood_value = 0\n",
    "\n",
    "    negative_exponential_contributions = np.empty(num_events, dtype=np.float64)\n",
    "\n",
    "    for n in range(num_events):\n",
    "        negative_exponential_contributions[n] = (\n",
    "            loglikelihood_negative_exponential_contribution(\n",
    "                alphas[n], betas[n], end_time, events_times[n]\n",
    "            )\n",
    "        )\n",
    "\n",
    "    for m in range(num_events):\n",
    "        loglikelihood_value += loglikelihood_m(\n",
    "            m,\n",
    "            mu[m],\n",
    "            negative_exponential_contributions[m],\n",
    "            alphas[m],\n",
    "            betas[m],\n",
    "            end_time,\n",
    "            events_times,\n",
    "        )\n",
    "\n",
    "    return loglikelihood_value\n",
    "\n",
    "\n",
    "@njit\n",
    "def l1_penalty(\n",
    "    alphas: np.ndarray, betas: np.ndarray, regularization_param: float\n",
    ") -> float:\n",
    "    return regularization_param * (np.sum(np.abs(alphas)) + np.sum(np.abs(betas)))\n",
    "\n",
    "\n",
    "@njit\n",
    "def instability_penalty(rhos: np.ndarray, instability_param: float) -> float:\n",
    "    kernel_spectral_norm = np.linalg.norm(rhos, ord=2)\n",
    "    if kernel_spectral_norm > 1:\n",
    "        return instability_param * kernel_spectral_norm\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[8.94947056e-03, 8.40306677e-02, 7.49080238e-02, 3.48352980e-03,\n",
       "        1.44619775e-01, 1.21508970e-01, 7.06857344e-01, 6.33403757e+01,\n",
       "        9.42909704e-01, 6.45172790e+01],\n",
       "       [3.63129734e-02, 6.71781780e-02, 1.90142861e-01, 1.64138590e-01,\n",
       "        3.29712762e-02, 3.41048247e-02, 3.25183322e+01, 2.49292229e-01,\n",
       "        3.23202932e-01, 8.35302496e-01],\n",
       "       [2.52182488e-03, 8.14232416e-02, 2.68682248e-01, 2.58544401e-01,\n",
       "        1.26730191e-01, 2.38775313e-02, 7.71270347e-01, 8.03672077e+01,\n",
       "        6.09564334e+01, 6.90937738e+01],\n",
       "       [6.66834962e-02, 7.75517037e-02, 1.42883068e-01, 4.28825844e-02,\n",
       "        1.03338589e-01, 2.26472488e-01, 6.37557471e+01, 7.55551139e-01,\n",
       "        5.02679023e+01, 3.86735346e+01],\n",
       "       [1.89771987e-02, 5.18179982e-02, 3.12037281e-02, 3.07703791e-02,\n",
       "        1.07798360e-01, 1.93126407e-01, 3.58465729e-01, 2.28798165e-01,\n",
       "        3.63629602e-01, 4.07751416e-02],\n",
       "       [4.85849675e-02, 7.98957670e-02, 1.03413450e-01, 1.02879070e-01,\n",
       "        6.15156118e-01, 5.35910870e-01, 4.72214925e+01, 7.69799098e-02,\n",
       "        2.78646464e+01, 1.37520944e+01],\n",
       "       [2.28588122e-02, 7.66935018e-03, 1.16167224e-02, 5.14871488e-02,\n",
       "        4.71956212e-02, 6.09227538e-02, 1.19594246e+01, 2.89751453e-01,\n",
       "        9.62447295e-01, 6.77564362e-01],\n",
       "       [3.81383216e-02, 1.69851814e-02, 1.73235229e-01, 8.88049346e-02,\n",
       "        1.21546322e-01, 1.95344228e-02, 6.23298127e-01, 8.96091300e+01,\n",
       "        2.51782296e-01, 1.13473521e+01],\n",
       "       [4.00920872e-02, 3.91969837e-03, 1.20223002e-01, 7.30983878e-02,\n",
       "        1.40025262e-01, 1.36846605e-01, 3.30898025e-01, 9.29697652e-01,\n",
       "        4.97248506e-01, 9.24693618e+01],\n",
       "       [1.35559934e-02, 2.81952953e-02, 3.22347929e-01, 1.12184090e-01,\n",
       "        2.49912140e-02, 2.00378110e-01, 5.61277198e+01, 1.10051925e+01,\n",
       "        3.00878310e-01, 8.77339353e+01]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "# 2 mu, 4 rho, 4 beta\n",
    "event_counts_for_type = [11, 13]\n",
    "initial_population_size = 10\n",
    "training_time_in_seconds = 30\n",
    "gene_upper_boundaries = [0.5, 0.5, 0.3, 0.3, 0.3, 0.3, 100, 100, 100, 100]\n",
    "\n",
    "type_count = len(event_counts_for_type)\n",
    "square_matrix_size = type_count * type_count\n",
    "parameters_count = 2 * square_matrix_size + type_count\n",
    "\n",
    "initial_population = np.zeros((initial_population_size, parameters_count))\n",
    "\n",
    "# fill the initial population with random rho values\n",
    "for i, j in [(i, j) for i in range(type_count) for j in range(type_count)]:\n",
    "    current_rho_max_value = min(\n",
    "        0.2 * event_counts_for_type[i] / event_counts_for_type[j], 0.5\n",
    "    )\n",
    "\n",
    "    initial_population[:, type_count + i * type_count + j] = np.random.uniform(\n",
    "        0,\n",
    "        min(\n",
    "            gene_upper_boundaries[type_count + i * type_count + j],\n",
    "            current_rho_max_value,\n",
    "        ),\n",
    "        initial_population_size,\n",
    "    )\n",
    "\n",
    "\n",
    "# fill the initial population with random mu values\n",
    "for i in range(type_count):\n",
    "    current_mu_max_value = 0.2 * event_counts_for_type[i] / training_time_in_seconds\n",
    "    initial_population[:, i] = np.random.uniform(\n",
    "        0,\n",
    "        min(gene_upper_boundaries[i], current_mu_max_value),\n",
    "        initial_population_size,\n",
    "    )\n",
    "\n",
    "\n",
    "# rescale the rho values for individuals with spectral norm > 1\n",
    "for individual_index in range(initial_population_size):\n",
    "    rho_values = initial_population[\n",
    "        individual_index, type_count : type_count + square_matrix_size\n",
    "    ].reshape((type_count, type_count))\n",
    "    kernel_spectral_norm = np.linalg.norm(rho_values, ord=2)\n",
    "    if kernel_spectral_norm > 1:\n",
    "        random_factor = np.random.uniform(0, 1)\n",
    "\n",
    "        rescaled_rho_matrix = rho_values * (random_factor / kernel_spectral_norm)\n",
    "\n",
    "        initial_population[\n",
    "            individual_index, type_count : type_count + square_matrix_size\n",
    "        ] = rescaled_rho_matrix.flatten()\n",
    "\n",
    "# fill the initial population with random beta values\n",
    "for i, j in [(i, j) for i in range(type_count) for j in range(type_count)]:\n",
    "    bernoulli_samples = np.random.binomial(n=1, p=0.5, size=initial_population_size)\n",
    "\n",
    "    random_values = np.where(\n",
    "        bernoulli_samples == 0,\n",
    "        np.random.uniform(0, 1, size=initial_population_size),\n",
    "        np.random.uniform(0, 100, size=initial_population_size),\n",
    "    )\n",
    "\n",
    "    initial_population[:, type_count + square_matrix_size + i * type_count + j] = (\n",
    "        random_values\n",
    "    )\n",
    "\n",
    "initial_population"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[8.94947056e-03, 8.40306677e-02, 7.49080238e-02, 3.48352980e-03,\n",
       "        1.44619775e-01, 1.21508970e-01, 7.72244769e-01, 3.14291857e-02,\n",
       "        9.62447295e+01, 6.77564362e+01],\n",
       "       [3.63129734e-02, 6.71781780e-02, 1.90142861e-01, 1.64138590e-01,\n",
       "        3.29712762e-02, 3.41048247e-02, 1.98715682e-01, 1.61221287e+01,\n",
       "        2.51782296e+01, 6.32305831e-01],\n",
       "       [2.52182488e-03, 8.14232416e-02, 1.46398788e+02, 1.40874908e+02,\n",
       "        6.90523715e+01, 1.30103186e+01, 3.30898025e+01, 9.29697652e+01,\n",
       "        1.19865367e-01, 6.33529711e-01],\n",
       "       [6.66834962e-02, 7.75517037e-02, 1.19731697e+02, 3.59343110e+01,\n",
       "        8.65946175e+01, 1.89777107e+02, 8.15461428e-01, 8.08120380e+01,\n",
       "        3.37615171e-01, 5.35774684e-01],\n",
       "       [1.89771987e-02, 5.18179982e-02, 3.12037281e-02, 3.07703791e-02,\n",
       "        1.07798360e-01, 1.93126407e-01, 7.06857344e-01, 6.33403757e+01,\n",
       "        9.42909704e-01, 6.45172790e+01],\n",
       "       [4.85849675e-02, 7.98957670e-02, 3.11989041e+01, 3.10376863e+01,\n",
       "        1.85587045e+02, 1.61679470e+02, 3.25183322e+01, 2.49292229e-01,\n",
       "        3.23202932e-01, 8.35302496e-01],\n",
       "       [2.28588122e-02, 7.66935018e-03, 1.16167224e-02, 5.14871488e-02,\n",
       "        4.71956212e-02, 6.09227538e-02, 7.71270347e-01, 8.03672077e+01,\n",
       "        6.09564334e+01, 6.90937738e+01],\n",
       "       [3.81383216e-02, 1.69851814e-02, 1.73235229e-01, 8.88049346e-02,\n",
       "        1.21546322e-01, 1.95344228e-02, 6.37557471e+01, 7.55551139e-01,\n",
       "        5.02679023e+01, 3.86735346e+01],\n",
       "       [4.00920872e-02, 3.91969837e-03, 1.20223002e-01, 7.30983878e-02,\n",
       "        1.40025262e-01, 1.36846605e-01, 3.58465729e-01, 2.28798165e-01,\n",
       "        3.63629602e-01, 4.07751416e-02],\n",
       "       [1.35559934e-02, 2.81952953e-02, 1.41614516e+02, 4.92849314e+01,\n",
       "        1.09791885e+01, 8.80304987e+01, 4.72214925e+01, 7.69799098e-02,\n",
       "        2.78646464e+01, 1.37520944e+01]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "initial_population"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "1\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "index = np.searchsorted(betas_mn, 2, side=\"left\")\n",
    "print(index)\n",
    "print(betas_mn[index - 1])\n",
    "print(betas_mn[index])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lshade",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
